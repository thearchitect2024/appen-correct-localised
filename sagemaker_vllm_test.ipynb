{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppenCorrect vLLM Testing on SageMaker\n",
    "\n",
    "**Instance Required:** ml.g5.xlarge (NVIDIA A10G GPU, 24GB VRAM)\n",
    "\n",
    "**What This Does:**\n",
    "1. Installs vLLM 0.6.3+ (better dependency management)\n",
    "2. Optionally installs Flash Attention 2 (2-3x speedup)\n",
    "3. Starts vLLM server with Qwen 2.5 7B Instruct\n",
    "4. Starts Flask API connected to vLLM\n",
    "5. Creates ngrok tunnel for public access\n",
    "6. Tests the complete system\n",
    "\n",
    "**Time:** ~15 min first run (downloads 14GB model), then <2 min on restarts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Flash Attention 2 (Optional)\n",
    "\n",
    "Flash Attention 2 provides 2-3x speedup. Skip if installation takes too long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installing Flash Attention 2 (5-10 min, provides 2-3x speedup)...')\n",
    "print('‚ö†Ô∏è  If this hangs, restart kernel and skip this step - vLLM works without it\\n')\n",
    "\n",
    "!pip install flash-attn --no-build-isolation\n",
    "\n",
    "print('\\n‚úÖ Flash Attention 2 installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install vLLM and Dependencies\n",
    "\n",
    "**Using vLLM 0.6.3+** which has better dependency management and avoids the `pyairports` issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean install - remove old packages that cause conflicts\n",
    "print('üßπ Cleaning old packages...')\n",
    "!pip uninstall vllm outlines pyairports -y 2>/dev/null || true\n",
    "\n",
    "print('\\n‚ö° Installing FlashInfer (CRITICAL for 2-3x speed improvement)...')\n",
    "# FlashInfer provides fast sampling operations for vLLM\n",
    "!pip install flashinfer -U\n",
    "\n",
    "print('\\nüì¶ Installing vLLM 0.6.3+ and dependencies...')\n",
    "# Install vLLM 0.6.3+ (better dependency management, no pyairports issues)\n",
    "!pip install vllm>=0.6.3 transformers torch pyngrok requests flask flask-cors python-dotenv jsonschema langdetect\n",
    "\n",
    "# Verify installation\n",
    "import subprocess\n",
    "result = subprocess.run(['pip', 'show', 'vllm'], capture_output=True, text=True)\n",
    "version_line = [line for line in result.stdout.split('\\n') if 'Version:' in line]\n",
    "\n",
    "flashinfer_result = subprocess.run(['pip', 'show', 'flashinfer'], capture_output=True, text=True)\n",
    "flashinfer_version = [line for line in flashinfer_result.stdout.split('\\n') if 'Version:' in line]\n",
    "\n",
    "print(f'\\n‚úÖ All dependencies installed')\n",
    "print(f'   {version_line[0] if version_line else \"vLLM version: unknown\"}')\n",
    "print(f'   {flashinfer_version[0] if flashinfer_version else \"‚ö†Ô∏è  FlashInfer NOT installed (will be slower!)\"}')\n",
    "print('   No pyairports conflicts!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Navigate to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to your cloned repo\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "!git checkout vllm\n",
    "!git pull origin vllm\n",
    "\n",
    "print(f'\\n‚úÖ Repository ready: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server (Background)\n",
    "\n",
    "**‚ö° Configuration for 4096 Context Window:**\n",
    "- **FlashInfer:** Enabled (2-3x faster sampling)\n",
    "- **Flash Attention:** Enabled (2-3x faster attention)\n",
    "- **max-model-len:** 4096 (handles ~11,900 chars / 2,000+ words)\n",
    "- **gpu-memory-utilization:** 90% (~20-21GB used on L4)\n",
    "- **max-num-seqs:** 8 (concurrent requests per GPU)\n",
    "- **Prefix caching:** Enabled (speeds up repeated system prompts)\n",
    "- **generation-config vllm:** **CRITICAL FIX** - Use vLLM's sampling config instead of model's creative defaults (prevents temperature=0.7 override)\n",
    "\n",
    "**Performance & Capacity:**\n",
    "- **Latency:** 3-6 seconds per request\n",
    "- **Concurrent Requests:** 8 per GPU node\n",
    "- **User Capacity:** ~80 active users per GPU (with typical usage patterns)\n",
    "- **Max Text:** ~11,900 characters (~2,000+ words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "# Set cache\n",
    "cache = '/home/sagemaker-user/.huggingface'\n",
    "os.makedirs(cache, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache\n",
    "\n",
    "# Kill existing\n",
    "!pkill -f vllm.entrypoints || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting vLLM server with 4096 context window...')\n",
    "print('‚è≥ First run: 5-10 min (downloads 14GB)')\n",
    "print('‚è≥ Next runs: 30-60 sec (from cache)\\n')\n",
    "print('‚ö° Configuration for L4 GPU (23GB VRAM):')\n",
    "print('  - max-model-len: 4096 (handles ~11,900 chars / 2,000+ words)')\n",
    "print('  - gpu-memory-utilization: 90%')\n",
    "print('  - max-num-seqs: 8 (concurrent requests per GPU)')\n",
    "print('  - FlashInfer: Enabled (2-3x faster sampling)')\n",
    "print('  - Flash Attention: Enabled (2-3x faster)')\n",
    "print('  - generation-config vllm: CRITICAL FIX for JSON output')\n",
    "print('\\nüí° Expected: 3-6s per request | Supports 80+ active users per GPU\\n')\n",
    "\n",
    "log_file = open('/tmp/vllm.log', 'w')\n",
    "vllm_process = subprocess.Popen([\n",
    "    'python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    '--host', '0.0.0.0', '--port', '8000',\n",
    "    '--dtype', 'auto',\n",
    "    '--max-model-len', '4096',  # UPGRADED: Handles up to 11,900 chars (~2,000 words)\n",
    "    '--gpu-memory-utilization', '0.90',  # 90% utilization for optimal performance\n",
    "    '--max-num-seqs', '8',  # REDUCED: 8 concurrent requests (larger KV cache per request)\n",
    "    '--enable-prefix-caching',\n",
    "    '--trust-remote-code',\n",
    "    '--generation-config', 'vllm',  # CRITICAL: Use vLLM's config, not model's creative defaults\n",
    "    '--disable-log-requests'\n",
    "], stdout=log_file, stderr=subprocess.STDOUT)\n",
    "\n",
    "for i in range(120):\n",
    "    try:\n",
    "        if requests.get('http://localhost:8000/health', timeout=2).status_code == 200:\n",
    "            print(f'\\n‚úÖ vLLM ready after {i*5}s!')\n",
    "            break\n",
    "    except:\n",
    "        if i % 6 == 0: print(f'  Loading... ({i*5}s)')\n",
    "        time.sleep(5)\n",
    "\n",
    "print('‚úÖ vLLM server running at http://localhost:8000')\n",
    "print('üìä Concurrent requests: 8 (per GPU node)')\n",
    "print('üìè Max sequence length: 4096 tokens (~11,900 chars)')\n",
    "print('üíæ GPU Memory: 90% utilization (~20-21GB used)')\n",
    "print('üë• User capacity: ~80 active users per GPU (with 3-5s latency)')\n",
    "print('\\nüìù Check logs: !tail -50 /tmp/vllm.log')\n",
    "print('üîç Check FlashInfer: !grep -i flashinfer /tmp/vllm.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test vLLM Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "r = requests.post('http://localhost:8000/v1/completions', json={\n",
    "    'model': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'prompt': 'Fix: I has a eror',\n",
    "    'max_tokens': 100, 'temperature': 0.2\n",
    "}, timeout=30)\n",
    "\n",
    "print('‚úÖ vLLM inference test:')\n",
    "print(r.json()['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Start Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "os.environ['VLLM_URL'] = 'http://localhost:8000'\n",
    "\n",
    "!pkill -f 'python.*app.py' || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting Flask API...')\n",
    "log_file = open('/tmp/flask.log', 'w')\n",
    "flask_process = subprocess.Popen(\n",
    "    ['python3', 'app.py'],\n",
    "    stdout=log_file,\n",
    "    stderr=subprocess.STDOUT\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "for i in range(10):\n",
    "    try:\n",
    "        r = requests.get('http://localhost:5006/health', timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print('\\n‚úÖ Flask API ready!')\n",
    "            print(r.json())\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "else:\n",
    "    print('‚ùå Flask failed to start. Check logs:')\n",
    "    print('!tail -50 /tmp/flask.log')\n",
    "\n",
    "print('\\nFlask running at http://localhost:5006')\n",
    "print('üìù Check logs: !tail -50 /tmp/flask.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://localhost:5006/demo/check', json={\n",
    "    'text': 'I has a eror in grammer'\n",
    "}, timeout=30)\n",
    "\n",
    "print(f'Status Code: {r.status_code}')\n",
    "result = r.json()\n",
    "\n",
    "if r.status_code == 200:\n",
    "    print('‚úÖ Grammar check test:\\n')\n",
    "    print(f'Original:  {result.get(\"original_text\", \"N/A\")}')\n",
    "    print(f'Corrected: {result.get(\"processed_text\", \"N/A\")}')\n",
    "    print(f'\\nCorrections: {len(result.get(\"corrections\", []))}')\n",
    "    for i, c in enumerate(result.get('corrections', [])[:5], 1):\n",
    "        print(f'  {i}. {c[\"type\"]}: \"{c[\"original\"]}\" ‚Üí \"{c[\"suggestion\"]}\"')\n",
    "    \n",
    "    stats = result.get('statistics', {})\n",
    "    print(f'\\nProcessing time: {stats.get(\"processing_time\", \"N/A\")}')\n",
    "    print(f'API type: {stats.get(\"api_type\", \"N/A\")}')\n",
    "else:\n",
    "    print(f'‚ùå Error: {result}')\n",
    "    print('\\nCheck Flask logs:')\n",
    "    print('!tail -100 /tmp/flask.log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create ngrok Tunnel\n",
    "\n",
    "**Get token:** https://dashboard.ngrok.com/get-started/your-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "\n",
    "NGROK_TOKEN = 'YOUR_TOKEN_HERE'  # ‚Üê CHANGE THIS!\n",
    "\n",
    "if NGROK_TOKEN == 'YOUR_TOKEN_HERE':\n",
    "    print('‚ö†Ô∏è  Set your ngrok token above!')\n",
    "    print('Get it: https://dashboard.ngrok.com/get-started/your-authtoken')\n",
    "else:\n",
    "    ngrok.kill()\n",
    "    conf.get_default().auth_token = NGROK_TOKEN\n",
    "    url = ngrok.connect(5006)\n",
    "    \n",
    "    print('='*70)\n",
    "    print('üöÄ PUBLIC URL:')\n",
    "    print('='*70)\n",
    "    print(f'\\n{url}\\n')\n",
    "    print(f'Demo:   {url}/')\n",
    "    print(f'Health: {url}/health')\n",
    "    print(f'API:    {url}/demo/check')\n",
    "    print('\\n' + '='*70)\n",
    "    print('\\nShare this URL to test from anywhere!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Get curl Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnels = ngrok.get_tunnels()\n",
    "if tunnels:\n",
    "    url = tunnels[0].public_url\n",
    "    print('Copy this curl command:\\n')\n",
    "    print(f'curl -X POST {url}/demo/check \\\\\\\\')\n",
    "    print('  -H \"Content-Type: application/json\" \\\\\\\\')\n",
    "    print('  -d \\'{\"text\": \"I has a eror in grammer\"}\\'\\n')\n",
    "    print(f'Or open: {url}/')\n",
    "else:\n",
    "    print('Run ngrok cell first!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f 'python.*app.py' || true\n",
    "!pkill -f vllm.entrypoints || true\n",
    "from pyngrok import ngrok\n",
    "ngrok.kill()\n",
    "print('‚úÖ Stopped all services')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Done!\n",
    "\n",
    "- **vLLM:** 0.6.3+ (better dependency management, no pyairports issues)\n",
    "- **Model:** Qwen 2.5 7B on L4 GPU (80% memory)\n",
    "- **Flask API:** Connected to vLLM\n",
    "- **ngrok:** Public access to full UI\n",
    "- **Concurrency:** 12 concurrent requests (testing)\n",
    "- **Max tokens:** 2048 (grammar checking with context)\n",
    "- **Model cache:** Persistent (no re-download)\n",
    "- **Cost:** $0.75/hr vs $3k-5k/month Gemini API\n",
    "\n",
    "### Production Settings (EKS):\n",
    "For 500+ users, scale to multiple GPUs:\n",
    "- **vLLM:** 0.6.3+ (stable, production-ready)\n",
    "- **GPU Memory:** 85% utilization\n",
    "- **Concurrent requests:** 32-64 per GPU\n",
    "- **Max tokens:** 4096\n",
    "- **Auto-scaling:** KEDA (pods) + Karpenter (GPU nodes)\n",
    "- **Spot instances:** 60-70% cost savings\n",
    "- **Flash Attention 2:** Recommended for production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
