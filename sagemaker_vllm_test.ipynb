{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppenCorrect vLLM Testing on SageMaker\n",
    "\n",
    "**Instance Required:** ml.g5.xlarge (NVIDIA A10G GPU, 24GB VRAM)\n",
    "\n",
    "This notebook:\n",
    "1. Installs vLLM + Flash Attention 2\n",
    "2. Starts vLLM server with Qwen 2.5 7B\n",
    "3. Starts Flask API\n",
    "4. Creates ngrok tunnel for public access\n",
    "5. Tests the complete system\n",
    "\n",
    "**Time:** ~15 min first run (downloads 14GB model), then 2 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Flash Attention 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Installing Flash Attention 2 (takes 5-10 min)...')\n",
    "!pip install flash-attn==2.6.3 --no-build-isolation\n",
    "print('\\n‚úÖ Flash Attention 2 installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install vLLM and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install vllm==0.6.3 transformers torch pyngrok requests flask flask-cors python-dotenv jsonschema langdetect\n",
    "print('\\n‚úÖ All dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Navigate to Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to your cloned repo\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "!git checkout vllm\n",
    "!git pull origin vllm\n",
    "\n",
    "print(f'\\n‚úÖ Repository ready: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Start vLLM Server (Background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "# Set cache\n",
    "cache = '/home/sagemaker-user/.huggingface'\n",
    "os.makedirs(cache, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache\n",
    "\n",
    "# Kill existing\n",
    "!pkill -f vllm.entrypoints || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting vLLM server...')\n",
    "print('‚è≥ First run: 5-10 min (downloads 14GB)')\n",
    "print('‚è≥ Next runs: 30-60 sec (from cache)\\n')\n",
    "\n",
    "vllm_process = subprocess.Popen([\n",
    "    'python', '-m', 'vllm.entrypoints.openai.api_server',\n",
    "    '--model', 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    '--host', '0.0.0.0', '--port', '8000',\n",
    "    '--dtype', 'auto',\n",
    "    '--max-model-len', '4096',\n",
    "    '--gpu-memory-utilization', '0.85',\n",
    "    '--max-num-seqs', '64',\n",
    "    '--enable-prefix-caching'\n",
    "], stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "for i in range(120):\n",
    "    try:\n",
    "        if requests.get('http://localhost:8000/health', timeout=2).status_code == 200:\n",
    "            print(f'\\n‚úÖ vLLM ready after {i*5}s!')\n",
    "            break\n",
    "    except:\n",
    "        if i % 6 == 0: print(f'  Loading... ({i*5}s)')\n",
    "        time.sleep(5)\n",
    "\n",
    "print('vLLM running at http://localhost:8000')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test vLLM Directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "r = requests.post('http://localhost:8000/v1/completions', json={\n",
    "    'model': 'Qwen/Qwen2.5-7B-Instruct',\n",
    "    'prompt': 'Fix: I has a eror',\n",
    "    'max_tokens': 100, 'temperature': 0.2\n",
    "}, timeout=30)\n",
    "\n",
    "print('‚úÖ vLLM inference test:')\n",
    "print(r.json()['choices'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Start Flask API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "os.environ['VLLM_URL'] = 'http://localhost:8000'\n",
    "\n",
    "!pkill -f 'python.*app.py' || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting Flask API...')\n",
    "flask_process = subprocess.Popen(\n",
    "    ['python3', 'app.py'],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "for i in range(10):\n",
    "    try:\n",
    "        r = requests.get('http://localhost:5006/health', timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print('\\n‚úÖ Flask API ready!')\n",
    "            print(r.json())\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "\n",
    "print('Flask running at http://localhost:5006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Test Complete System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://localhost:5006/demo/check', json={\n",
    "    'text': 'I has a eror in grammer'\n",
    "}, timeout=30)\n",
    "\n",
    "result = r.json()\n",
    "print('‚úÖ Grammar check test:\\n')\n",
    "print(f'Original:  {result[\"original_text\"]}')\n",
    "print(f'Corrected: {result[\"corrected_text\"]}')\n",
    "print(f'\\nErrors: {len(result[\"errors\"])}')\n",
    "for e in result['errors'][:3]:\n",
    "    print(f'  {e[\"original\"]} ‚Üí {e[\"suggestion\"]} ({e[\"type\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Create ngrok Tunnel\n",
    "\n",
    "**Get token:** https://dashboard.ngrok.com/get-started/your-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "\n",
    "NGROK_TOKEN = 'YOUR_TOKEN_HERE'  # ‚Üê CHANGE THIS!\n",
    "\n",
    "if NGROK_TOKEN == 'YOUR_TOKEN_HERE':\n",
    "    print('‚ö†Ô∏è  Set your ngrok token above!')\n",
    "    print('Get it: https://dashboard.ngrok.com/get-started/your-authtoken')\n",
    "else:\n",
    "    ngrok.kill()\n",
    "    conf.get_default().auth_token = NGROK_TOKEN\n",
    "    url = ngrok.connect(5006)\n",
    "    \n",
    "    print('='*70)\n",
    "    print('üöÄ PUBLIC URL:')\n",
    "    print('='*70)\n",
    "    print(f'\\n{url}\\n')\n",
    "    print(f'Demo:   {url}/')\n",
    "    print(f'Health: {url}/health')\n",
    "    print(f'API:    {url}/demo/check')\n",
    "    print('\\n' + '='*70)\n",
    "    print('\\nShare this URL to test from anywhere!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Get curl Command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnels = ngrok.get_tunnels()\n",
    "if tunnels:\n",
    "    url = tunnels[0].public_url\n",
    "    print('Copy this curl command:\\n')\n",
    "    print(f'curl -X POST {url}/demo/check \\\\\\\\')\n",
    "    print('  -H \"Content-Type: application/json\" \\\\\\\\')\n",
    "    print('  -d \\'{\"text\": \"I has a eror in grammer\"}\\'\\n')\n",
    "    print(f'Or open: {url}/')\n",
    "else:\n",
    "    print('Run ngrok cell first!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitor GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f 'python.*app.py' || true\n",
    "!pkill -f vllm.entrypoints || true\n",
    "from pyngrok import ngrok\n",
    "ngrok.kill()\n",
    "print('‚úÖ Stopped all services')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Done!\n",
    "\n",
    "- vLLM server: Qwen 2.5 7B on GPU\n",
    "- Flask API: Connected to vLLM\n",
    "- ngrok: Public access\n",
    "- Concurrency: 64 requests/GPU\n",
    "- Model: Cached (no re-download)\n",
    "- Cost: $0.75/hr vs $3k-5k/month API"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
