{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AppenCorrect TGI Testing on SageMaker\n",
    "\n",
    "**Instance Required:** ml.g5.xlarge or g6.xlarge (NVIDIA GPU, 24GB VRAM)\n",
    "\n",
    "This notebook:\n",
    "1. Installs TGI (Text-Generation-Inference)\n",
    "2. Starts TGI server with Qwen 2.5 7B\n",
    "3. Starts Flask API\n",
    "4. Creates ngrok tunnel for public access\n",
    "5. Tests the complete system\n",
    "\n",
    "**Time:** ~5 min first run (downloads 14GB model), then 1 min"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 1: Check GPU"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ["!nvidia-smi"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 2: Install TGI Client and Dependencies"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install text-generation aiohttp transformers torch pyngrok requests flask flask-cors python-dotenv jsonschema langdetect\n",
    "print('\\n‚úÖ All dependencies installed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 3: Clone Repository"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Navigate to your cloned repo\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "!git checkout tgi\n",
    "!git pull origin tgi\n",
    "\n",
    "print(f'\\n‚úÖ Repository ready: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Start TGI Server (Docker)\n",
    "\n",
    "**Note:** TGI runs best in Docker. We'll use the official HuggingFace container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "# Set cache\n",
    "cache = '/home/sagemaker-user/.huggingface'\n",
    "os.makedirs(cache, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache\n",
    "\n",
    "# Kill existing\n",
    "!docker stop tgi-server 2>/dev/null || true\n",
    "!docker rm tgi-server 2>/dev/null || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting TGI server with Docker...')\n",
    "print('‚è≥ First run: 3-5 min (downloads 14GB)')\n",
    "print('‚è≥ Next runs: 30-60 sec (from cache)\\n')\n",
    "print('Memory Settings: Optimized for 24GB VRAM GPU')\n",
    "print('  - Model: ~14GB')\n",
    "print('  - KV Cache: ~8GB')\n",
    "print('  - Reserved: ~2GB\\n')\n",
    "\n",
    "# Start TGI in Docker (background)\n",
    "!docker run -d \\\\\n",
    "    --name tgi-server \\\\\n",
    "    --gpus all \\\\\n",
    "    -p 8080:80 \\\\\n",
    "    -v {cache}:/data \\\\\n",
    "    ghcr.io/huggingface/text-generation-inference:latest \\\\\n",
    "    --model-id Qwen/Qwen2.5-7B-Instruct \\\\\n",
    "    --max-concurrent-requests 8 \\\\\n",
    "    --max-input-length 512 \\\\\n",
    "    --max-total-tokens 1536 \\\\\n",
    "    --dtype auto\n",
    "\n",
    "# Wait for TGI to be ready\n",
    "print('Waiting for TGI server...')\n",
    "for i in range(60):\n",
    "    try:\n",
    "        r = requests.get('http://localhost:8080/health', timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print(f'\\n‚úÖ TGI ready after {i*5}s!')\n",
    "            break\n",
    "    except:\n",
    "        if i % 3 == 0: print(f'  Loading... ({i*5}s)')\n",
    "        time.sleep(5)\n",
    "else:\n",
    "    print('‚ùå Timeout waiting for TGI. Check logs: !docker logs tgi-server')\n",
    "\n",
    "print('\\n‚úÖ TGI server running at http://localhost:8080')\n",
    "print('üìä Concurrent requests: 8 (enough for testing)')\n",
    "print('üìè Max tokens: 1536 (512 input + 1024 output)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 5: Test TGI Directly"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "r = requests.post('http://localhost:8080/generate', json={\n",
    "    'inputs': 'Fix: I has a eror',\n",
    "    'parameters': {'max_new_tokens': 100, 'temperature': 0.2}\n",
    "}, timeout=30)\n",
    "\n",
    "print('‚úÖ TGI inference test:')\n",
    "print(r.json()['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 6: Start Flask API"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, time, requests, os\n",
    "\n",
    "os.chdir('/home/sagemaker-user/appen-correct-localised')\n",
    "os.environ['TGI_URL'] = 'http://localhost:8080'\n",
    "\n",
    "!pkill -f 'python.*app.py' || true\n",
    "time.sleep(2)\n",
    "\n",
    "print('üöÄ Starting Flask API...')\n",
    "flask_process = subprocess.Popen(\n",
    "    ['python3', 'app.py'],\n",
    "    stdout=subprocess.DEVNULL,\n",
    "    stderr=subprocess.DEVNULL\n",
    ")\n",
    "\n",
    "time.sleep(5)\n",
    "for i in range(10):\n",
    "    try:\n",
    "        r = requests.get('http://localhost:5006/health', timeout=2)\n",
    "        if r.status_code == 200:\n",
    "            print('\\n‚úÖ Flask API ready!')\n",
    "            print(r.json())\n",
    "            break\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "\n",
    "print('Flask running at http://localhost:5006')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 7: Test Complete System"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post('http://localhost:5006/demo/check', json={\n",
    "    'text': 'I has a eror in grammer'\n",
    "}, timeout=30)\n",
    "\n",
    "result = r.json()\n",
    "print('‚úÖ Grammar check test:\\n')\n",
    "print(f'Original:  {result[\"original_text\"]}')\n",
    "print(f'Corrected: {result[\"corrected_text\"]}')\n",
    "print(f'\\nErrors: {len(result[\"errors\"])}')\n",
    "for e in result['errors'][:3]:\n",
    "    print(f'  {e[\"original\"]} ‚Üí {e[\"suggestion\"]} ({e[\"type\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Create ngrok Tunnel\n",
    "\n",
    "**Get token:** https://dashboard.ngrok.com/get-started/your-authtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyngrok import ngrok, conf\n",
    "\n",
    "NGROK_TOKEN = 'YOUR_TOKEN_HERE'  # ‚Üê CHANGE THIS!\n",
    "\n",
    "if NGROK_TOKEN == 'YOUR_TOKEN_HERE':\n",
    "    print('‚ö†Ô∏è  Set your ngrok token above!')\n",
    "    print('Get it: https://dashboard.ngrok.com/get-started/your-authtoken')\n",
    "else:\n",
    "    ngrok.kill()\n",
    "    conf.get_default().auth_token = NGROK_TOKEN\n",
    "    url = ngrok.connect(5006)\n",
    "    \n",
    "    print('='*70)\n",
    "    print('üöÄ PUBLIC URL:')\n",
    "    print('='*70)\n",
    "    print(f'\\n{url}\\n')\n",
    "    print(f'Demo:   {url}/')\n",
    "    print(f'Health: {url}/health')\n",
    "    print(f'API:    {url}/demo/check')\n",
    "    print('\\n' + '='*70)\n",
    "    print('\\nShare this URL to test from anywhere!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 9: Get curl Command"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tunnels = ngrok.get_tunnels()\n",
    "if tunnels:\n",
    "    url = tunnels[0].public_url\n",
    "    print('Copy this curl command:\\n')\n",
    "    print(f'curl -X POST {url}/demo/check \\\\\\\\')\n",
    "    print('  -H \"Content-Type: application/json\" \\\\\\\\')\n",
    "    print('  -d \\'{\"text\": \"I has a eror in grammer\"}\\'\\n')\n",
    "    print(f'Or open: {url}/')\n",
    "else:\n",
    "    print('Run ngrok cell first!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Monitor GPU"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-gpu=utilization.gpu,memory.used,memory.total --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Cleanup"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pkill -f 'python.*app.py' || true\n",
    "!docker stop tgi-server || true\n",
    "from pyngrok import ngrok\n",
    "ngrok.kill()\n",
    "print('‚úÖ Stopped all services')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Done!\n",
    "\n",
    "- **TGI server:** Qwen 2.5 7B on GPU (Docker)\n",
    "- **Flask API:** Connected to TGI\n",
    "- **ngrok:** Public access to full UI\n",
    "- **Concurrency:** 8 concurrent requests\n",
    "- **Model:** Cached (no re-download)\n",
    "- **Cost:** $0.75/hr vs $3k-5k/month Gemini API\n",
    "- **NO pyairports issues!** ‚ú®"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
